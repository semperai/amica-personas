# llama.cpp Server Configuration

# Port to expose
LLAMACPP_PORT=8081

# Model file path (relative to ./models/ directory)
# Download GGUF models from https://huggingface.co/models?search=gguf
LLAMACPP_MODEL=mistral-7b-instruct-v0.2.Q4_K_M.gguf

# Context size (tokens)
LLAMACPP_CONTEXT=2048

# Number of CPU threads
LLAMACPP_THREADS=4

# GPU layers (0 = CPU only, increase for GPU acceleration)
LLAMACPP_GPU_LAYERS=0

# Batch size
LLAMACPP_BATCH=512
