version: '3.8'

services:
  llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: amica-llamacpp
    ports:
      - "${LLAMACPP_PORT:-8081}:8080"
    volumes:
      - ./models:/models
    command: [
      "-m", "/models/${LLAMACPP_MODEL:-mistral-7b-instruct-v0.2.Q4_K_M.gguf}",
      "--host", "0.0.0.0",
      "--port", "8080",
      "-c", "${LLAMACPP_CONTEXT:-2048}",
      "-t", "${LLAMACPP_THREADS:-4}",
      "-ngl", "${LLAMACPP_GPU_LAYERS:-0}",
      "-b", "${LLAMACPP_BATCH:-512}"
    ]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
