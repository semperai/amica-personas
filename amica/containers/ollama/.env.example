# Ollama Configuration

# Port to expose
OLLAMA_PORT=11434

# Host binding
OLLAMA_HOST=0.0.0.0

# Default models to pull on first start (comma-separated, optional)
# Examples: llama2, mistral, llama3.2-vision, codellama
# OLLAMA_MODELS=llama2,mistral

# GPU settings (optional)
# OLLAMA_GPU_LAYERS=35
